{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Create_Own_pkl_file.ipynb file to get the pkl file used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def reduce_memory(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "        if df[col].dtype == 'int64':\n",
    "            df[col] = df[col].astype('int32')\n",
    "    return df\n",
    "\n",
    "games = reduce_memory(pd.read_csv('./final_dataset.csv'))\n",
    "n_recommendation = 20\n",
    "vectors = pickle.load(open(\"./vectors_final.pkl\",'rb'))\n",
    "\n",
    "# similarity=pickle.load(open(\"./similarity_forDesc.pkl\",'rb'))\n",
    "\n",
    "# def recommend(game):\n",
    "#     index = games[games['title'] == game].index[0]\n",
    "#     sim_scores = sorted(list(enumerate(similarity[index])),reverse=True,key = lambda x: x[1])\n",
    "#     print(sim_scores)\n",
    "#     game_lists=[]\n",
    "#     for i in sim_scores[1:n_recommendation]:\n",
    "#         game_lists.append(games.iloc[i[0]].title)\n",
    "#     return game_lists\n",
    "def recommend(game):\n",
    "    index = games[games['title'] == game].index[0]\n",
    "\n",
    "    item_vector = vectors[index]\n",
    "    similarities = cosine_similarity(item_vector, vectors).flatten()\n",
    "    recommended_indices = similarities.argsort()[::-1]  # Get the indices of the most similar items\n",
    "    game_lists=[]\n",
    "    for i in recommended_indices[1:n_recommendation]:\n",
    "        game_lists.append(games.iloc[i].title)\n",
    "    return (game_lists)\n",
    "\n",
    "# recommend('God Sword')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18795299 0.29029426 0.32891773 ... 0.04082653 0.08461538 0.04482858]\n",
      "[43458 43770 43459 ... 20000 19980 19987]\n",
      "[1.         0.61782654 0.52787967 ... 0.         0.         0.        ]\n",
      "Call of Duty®: Modern Warfare®\n",
      "Call of Duty®: Vanguard\n",
      "The Undisputables : Online Multiplayer Shooter\n",
      "Tactical Operations Force\n",
      "Call of Duty®: Modern Warfare® II\n",
      "Cyberfrags '69\n",
      "Burst Squad\n",
      "Object N\n",
      "Men of War: Assault Squad 2 - Cold War\n",
      "World Of Stalkers\n",
      "World of Warplanes\n",
      "Warplanes: Battles over Pacific\n",
      "Men of War: Vietnam\n",
      "Modern Assault Tanks\n",
      "The Last War\n",
      "Battlefield 4™\n",
      "Battle Strike World War\n",
      "Poly Squad\n",
      "PIRATECRAFT\n"
     ]
    }
   ],
   "source": [
    "recommend(\"Call of Duty®: Black Ops Cold War\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Call of Duty®',\n",
       " 'Call of Duty: United Offensive',\n",
       " 'Call of Duty®: Modern Warfare® Remastered (2017)',\n",
       " 'Warrior Paint - 2005 GOTY Edition',\n",
       " 'Verdun',\n",
       " 'Red Orchestra: Ostfront 41-45',\n",
       " 'Fog Of War - Free Edition',\n",
       " 'Day of Defeat: Source',\n",
       " 'Battlefield: Bad Company 2 Vietnam',\n",
       " 'Day of Infamy',\n",
       " 'Battle For The Sun',\n",
       " 'Red Orchestra 2: Heroes of Stalingrad with Rising Storm',\n",
       " 'Operation Flashpoint: Dragon Rising',\n",
       " 'Call of Duty® 4: Modern Warfare® (2007)',\n",
       " 'Toy Soldiers',\n",
       " 'World of Warplanes',\n",
       " 'Battle Islands',\n",
       " 'Medal of Honor™',\n",
       " 'Land of War - The Beginning']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend(\"Call of Duty® 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_game_recommendations(game_name):\n",
    "    n=10\n",
    "    game_similarity_df = pickle.load(open(\"./game_similarity_df.pkl\",'rb'))\n",
    "    mapped_df = pickle.load(open(\"./mapped_df.pkl\",'rb'))\n",
    "    # Check if the game name exists in the filtered_df_player_count\n",
    "    if game_name not in mapped_df['title'].values:\n",
    "        print(\"The game lacks rating. Unable to find recommendations.\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    # Get the app_id of the input game\n",
    "    app_id = mapped_df.loc[mapped_df['title'] == game_name, 'app_id'].values[0]\n",
    "    \n",
    "    # Check if the app_id is in the similarity matrix\n",
    "    if app_id not in game_similarity_df.index:\n",
    "        print(\"No similar games found/ Low rated game \")\n",
    "        return\n",
    "    \n",
    "    # Get similarity scores for the game and sort them in descending order\n",
    "    similarity_scores = game_similarity_df.loc[app_id].sort_values(ascending=False)\n",
    "    \n",
    "    # Get top n similar app_ids (excluding the game itself)\n",
    "    top_app_ids = similarity_scores.iloc[1:n+1].index  # Exclude the first as it's the game itself\n",
    "    \n",
    "    # Map app_ids to game titles\n",
    "    recommendations = mapped_df[mapped_df['app_id'].isin(top_app_ids)]['title'].tolist()\n",
    "    \n",
    "    return (recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = {\n",
    "    \"Call of Duty® 2\": [\"Call of Duty®\", \"Call of Duty®: Modern Warfare® Remastered (2017)\", \n",
    "                        \"Battlefield: Bad Company 2 Vietnam\", \"Call of Duty: United Offensive\",\"Verdun\"],\n",
    "    \"Half-Life 2: Episode One\": [\"Half-Life 2\",\"Half-Life 2: Episode Two\", \"Half-Life: Source\",\"Half-Life 2: Update\"],  # Relevant items for Half-Life\n",
    "    \"Battlefield: Bad Company™ 2\": [\"Battlefield 4™\",\"Battlefield™ 2042\",\"World War 3\",\"Vanguard: Normandy 1944\",\"Battlefield: Bad Company 2 Vietnam\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall(input_title, recommended_titles, ground_truth, top_n=3):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, and other metrics for a single input title.\n",
    "    \"\"\"\n",
    "    # Get the relevant items from the ground truth for this test item\n",
    "    relevant_items = ground_truth.get(input_title, [])\n",
    "    if not relevant_items:\n",
    "        return {\"precision\": 0, \"recall\": 0, \"true_positives\": 0, \"false_positives\": 0, \"false_negatives\": 0}\n",
    "    # Consider only the top N recommended items\n",
    "    top_recommended = recommended_titles[:top_n]\n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    true_positives = len(set(relevant_items) & set(top_recommended))\n",
    "    false_positives = len(set(top_recommended) - set(relevant_items))\n",
    "    false_negatives = len(set(relevant_items) - set(top_recommended))\n",
    "    # Calculate precision and recall\n",
    "    precision = true_positives / len(top_recommended) if len(top_recommended) > 0 else 0\n",
    "    recall = true_positives / len(relevant_items) if len(relevant_items) > 0 else 0\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"true_positives\": true_positives,\n",
    "        \"false_positives\": false_positives,\n",
    "        \"false_negatives\": false_negatives,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['Half-Life 2', 'Half-Life 2: Episode One', 'Half-Life 2: Episode Two', 'Portal', 'Team Fortress 2', 'Left 4 Dead', 'Portal 2']\n",
      "['Half-Life 2', 'Counter-Strike: Source', 'Half-Life 2: Episode One', 'Portal', 'Half-Life 2: Episode Two', 'Team Fortress 2', 'Left 4 Dead', 'Portal 2']\n",
      "Recommendations for 'Half-Life':\n",
      "['Half-Life 2', 'Counter-Strike: Source', 'Half-Life 2: Episode One', 'Portal', 'Half-Life 2: Episode Two', 'Team Fortress 2', 'Left 4 Dead', 'Portal 2', 'POSTAL 2', 'Black Mesa']\n",
      "Precision: 0.875\n",
      "Recall: 1.0\n",
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "input_title = \"Half-Life\"\n",
    "# recommended_titles = recommend(input_title)\n",
    "recommended_titles = get_game_recommendations(\"Half-Life\")\n",
    "\n",
    "print(type(recommended_titles));\n",
    "\n",
    "# Calculate precision and recall for the recommendation\n",
    "metrics = calculate_precision_recall(input_title, recommended_titles, ground_truth, top_n=8)\n",
    "\n",
    "print(f\"Recommendations for '{input_title}':\")\n",
    "print(recommended_titles)\n",
    "print(f\"Precision: {metrics['precision']}\")\n",
    "print(f\"Recall: {metrics['recall']}\")\n",
    "precision = metrics['precision']\n",
    "recall = metrics['recall']\n",
    "\n",
    "# Calculate F1 score\n",
    "if precision + recall > 0:\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "else:\n",
    "    f1_score = 0.0\n",
    "\n",
    "print(\"Fi Score:\", f1_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8\n",
      "Recall: 0.8571428571428571\n",
      "Averaged F1 Score: 0.8275862068965518\n"
     ]
    }
   ],
   "source": [
    "all_titles = [\"Call of Duty® 2\", \"Half-Life 2: Episode One\",\"Battlefield: Bad Company™ 2\"]\n",
    "f1_scores = []\n",
    "total_tp, total_fp, total_fn = 0, 0, 0  # For micro-average\n",
    "for input_title in all_titles:\n",
    "    # Get recommendations\n",
    "    recommended_titles = recommend(input_title)\n",
    "    # Calculate precision and recall\n",
    "    metrics = calculate_precision_recall(input_title, recommended_titles, ground_truth, top_n=5)\n",
    "    precision = metrics['precision']\n",
    "    recall = metrics['recall']\n",
    "    # Calculate F1 score for this title\n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "    # Add F1 score to list for macro-average\n",
    "    f1_scores.append(f1)\n",
    "    # Aggregate TP, FP, FN for micro-average\n",
    "    total_tp += metrics['true_positives']\n",
    "    total_fp += metrics['false_positives']\n",
    "    total_fn += metrics['false_negatives']\n",
    "# Micro-Averaged F1\n",
    "if total_tp + total_fp > 0 and total_tp + total_fn > 0:\n",
    "    micro_precision = total_tp / (total_tp + total_fp)\n",
    "    micro_recall = total_tp / (total_tp + total_fn)\n",
    "    micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall)\n",
    "else:\n",
    "    micro_f1 = 0.0\n",
    "# print(f\"Macro-Averaged F1: {macro_f1}\")\n",
    "print(f\"Precision: {micro_precision}\" )\n",
    "print(f\"Recall: {micro_recall}\" )\n",
    "print(f\"Averaged F1 Score: {micro_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Half-Life 2',\n",
       " 'Counter-Strike: Source',\n",
       " 'Half-Life 2: Episode One',\n",
       " 'Portal',\n",
       " 'Half-Life 2: Episode Two',\n",
       " 'Team Fortress 2',\n",
       " 'Left 4 Dead',\n",
       " 'Portal 2',\n",
       " 'POSTAL 2',\n",
       " 'Black Mesa']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
